{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tasdelen_Analyse_de_sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/btasde42/BertSentimentAnalysis/blob/master/Tasdelen_Analyse_de_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNH1zhJv2xI2"
      },
      "source": [
        "# **DEEP LEARNING MODEL FOR SENTIMENT ANALIYSIS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGcXqnA02ND4"
      },
      "source": [
        "## **Preprocessing data and train/valid set split**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIGx6iULCKbH",
        "outputId": "a9f63788-cabd-4f33-d2d7-5c3e4698341c"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk #NLP library.\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNs0oj8KShKG",
        "outputId": "5410fb89-f8f1-43a2-e551-4070fae80ed1"
      },
      "source": [
        "#Check if GPU is avaible then activate GPU, else activate cpu\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuovvCPpL5ma"
      },
      "source": [
        "#upload file from files section in left\n",
        "datafile='dataset.txt' #define main dataset\n",
        "\n",
        "def preprocess(text):\n",
        "  \"\"\"Function to preprocess text input: lowercase, special char removing, stopword removing\n",
        "  Args:\n",
        "    text: str\n",
        "  Returns:\n",
        "    text: modified str\"\"\"\n",
        "  text=text.lower() #put strings on lower\n",
        "  text=text.replace(\"\\\\n\",'') #remove \"\\\\n\" char from text\n",
        "  text=text.replace(\"\\'\",'') #remove \"\\'\" char from text\n",
        "  text=text.replace(\"\\n\",'') #remove \"\\n\" char from text\n",
        "  text=text.replace('\"','') #remove '\"' char from text\n",
        "  text=re.sub(r'\\s+', ' ', text).strip() #unify space between words\n",
        "  \n",
        "  pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*') #pattern for stopwords\n",
        "  text = pattern.sub('', text) #remove english stopwords\n",
        "  \n",
        "  return text \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def read_data_file(datafile,labelled=True):\n",
        "  \"\"\"File reader function\n",
        "  Args:\n",
        "    datafile: labelled or non labelled text file\n",
        "    labelled: boolean flag for labelled text\n",
        "  \"\"\"\n",
        "\n",
        "  list_data=[] #if labelled data in form of [label,[token_list]] else [str]\n",
        "  with open(datafile, 'r') as f: #get reviews from datafile\n",
        "    next(f) #skip headers\n",
        "    for lines in f:\n",
        "      if labelled==True:\n",
        "        label, text =lines.split('\\t')[0], lines.split('\\t')[1]\n",
        "        preprocessed_text=preprocess(text) #preprocess text\n",
        "        if int(label)==1: #transforming classes to [0;1]\n",
        "          label=0\n",
        "        else:\n",
        "          label=1\n",
        "        preprocessed_text=[label, preprocessed_text] #return a list of list [[label,text]]\n",
        "      else:\n",
        "        preprocessed_text=preprocess(lines)\n",
        "      list_data.append(preprocessed_text)    \n",
        "  return list_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7zLQFgDgjoV",
        "outputId": "7de7e2ad-6c93-486e-e5f7-84478e758e70"
      },
      "source": [
        "#Creating balanced valid-train-test sets\n",
        "df_data=pd.DataFrame(read_data_file(datafile),columns=['label','text'])\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(df_data['text'],\n",
        "                                               df_data['label'],\n",
        "                                               test_size=0.30,\n",
        "                                               shuffle=True,\n",
        "                                               stratify=df_data['label']) #test-train split from data with balanced classes\n",
        "\n",
        "x_test,x_valid,y_test,y_valid=train_test_split(x_test,\n",
        "                                               y_test,test_size=0.50,\n",
        "                                               shuffle=True,\n",
        "                                               stratify=y_test) #train-valid split from train with balenced classes\n",
        "print(\"Train label counts:\")\n",
        "print(y_train.value_counts())\n",
        "print()\n",
        "print(\"Test label counts:\")\n",
        "print(y_test.value_counts())\n",
        "print()\n",
        "print(\"Validation label counts:\")\n",
        "print(y_valid.value_counts())\n",
        "\n",
        "#create for each dataset text and label values in separated lists \n",
        "train_x=x_train.astype(str).tolist()\n",
        "train_y=y_train.astype(int).tolist()\n",
        "\n",
        "valid_x=x_valid.astype(str).tolist()\n",
        "valid_y=y_valid.astype(int).tolist()\n",
        "\n",
        "test_x=x_test.astype(str).tolist()\n",
        "test_y=y_test.astype(int).tolist()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train label counts:\n",
            "0    18782\n",
            "1    16218\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Test label counts:\n",
            "0    4025\n",
            "1    3475\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Validation label counts:\n",
            "0    4024\n",
            "1    3476\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b6vdUbYEkIH"
      },
      "source": [
        "## **Set Bert tokenizer**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJtfsrzo8sub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e11a68-8596-4ec3-da66-c2083550f879"
      },
      "source": [
        "#installing tokenizer transformers package\n",
        "print(\"Installing BertModel, BertConfig and BertTokenizer!\")\n",
        "try: #if transformers model already installed\n",
        "  from transformers import BertModel, BertConfig, BertTokenizer\n",
        "except ImportError: #else\n",
        "  !pip install transformers #install transformers\n",
        "  from transformers import BertModel, BertConfig, BertTokenizer\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing BertModel, BertConfig and BertTokenizer!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5am3S2-XAA3"
      },
      "source": [
        "# charge tokenizer from bert-base-uncased model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRcirXs4XdQt"
      },
      "source": [
        "def tokenize_Bert(dataset,MAX_LEN):\n",
        "  \"\"\"Function to implement bert tokenization/truncation\n",
        "  Args:\n",
        "    dataset:list of reviews (list(str))\n",
        "  Returns:\n",
        "    return tokenized/truncated reviews\n",
        "    \"\"\"\n",
        "  all_ids=[]\n",
        "  #print(len(dataset))\n",
        "  for review in dataset:\n",
        "    review_encodings = tokenizer.encode(review,\n",
        "                                    add_special_tokens=True, #add all special tokens\n",
        "                                    truncation=True, #truncate longer reviews than max_len\n",
        "                                    max_length = MAX_LEN, \n",
        "                                    pad_to_max_length=False) #disable padding for this step\n",
        "                                    \n",
        "    all_ids.append(review_encodings)\n",
        "    \n",
        "  return all_ids\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwzpkfDiQcjY"
      },
      "source": [
        "## **Smart batch creation for datasets**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd794jS-u99o"
      },
      "source": [
        "def pad_batches(batch_reviews,batch_labels):\n",
        "  \"\"\"Padding funtion on already selected batches\n",
        "  Args:\n",
        "    batch_reviews: list of lists (str)\n",
        "    batch_labels: list of lists (int)\n",
        "  Returns:\n",
        "    list of tensor values\n",
        "  \"\"\"\n",
        "    #lists to transform to tensors for output values\n",
        "  tensor_inputs = []\n",
        "  tensor_attention_masks = []\n",
        "  tensor_labels = []\n",
        "\n",
        "  for (batch_sent,batch_labels) in zip(batch_reviews,batch_labels): #iteration on both sentence and label batch\n",
        "    \n",
        "    batch_padded_sentences=[]\n",
        "    batch_attention_masks=[] #we're creating attention mask list\n",
        "\n",
        "    #for each batch we need to find longest sentence to pad other sentences to its lenght\n",
        "    max_len=max(len(sent) for sent in batch_sent)\n",
        "    \n",
        "    # iterate over the values of the current batch\n",
        "    for sent in batch_sent:\n",
        "\n",
        "      #pad sentence \n",
        "      padded_sent = sent + [tokenizer.pad_token_id]*(max_len-(len(sent)))\n",
        "      \n",
        "      #create attention mask; pad values should take 0 and all other values 1\n",
        "      attion_mask = [1] * len(sent) + [0] * (max_len-(len(sent)))\n",
        "      \n",
        "      # Add the padded results to the batch.\n",
        "      batch_padded_sentences.append(padded_sent)\n",
        "      batch_attention_masks.append(attion_mask)\n",
        "\n",
        "    #transform padded input batches, newly created attion mask and labels to tensors and add to list\n",
        "    tensor_inputs.append(torch.tensor(batch_padded_sentences))\n",
        "    tensor_attention_masks.append(torch.tensor(batch_attention_masks))\n",
        "    tensor_labels.append(torch.tensor(batch_labels))\n",
        "  \n",
        "  return [tensor_inputs,tensor_attention_masks,tensor_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1Ximu3_dVhD"
      },
      "source": [
        "import random\n",
        "\n",
        "def select_batches(review_list,label_list,batch_size):\n",
        "  \n",
        "  \"\"\"Smart batching technique for bert to reduce training time\n",
        "  Args:\n",
        "    review_list: list of str for extracted reviews\n",
        "    label_list: list of int for extracted labels\n",
        "    batch_size: number of examples in each batch\n",
        "  Returns:\n",
        "     a list of batches :tensor_inputs(len of batch size), tensor attention mask and tensor labels for\n",
        "    \"\"\"\n",
        "\n",
        "  input_ids=tokenize_Bert(review_list,500) #generate tokenization on text dataset\n",
        "  examples = sorted(zip(input_ids, label_list), key=lambda x: len(x[0])) #sort examples by the length of reviews in input_ids\n",
        "\n",
        "  # List of batches that we'll construct.\n",
        "  batch_ordered_sentences = []\n",
        "  batch_ordered_labels = []\n",
        "  \n",
        "  while len(examples) > 0: #iterate untill no exemple remains \n",
        "\n",
        "    #if take minimum exemple size if batch_size not smaller\n",
        "    batch_s=min(batch_size,len(examples)) #new batch size\n",
        "    \n",
        "    random_ind = random.randint(0, len(examples) - batch_s) #choose a random index in examples for first sentence of batch\n",
        "    #create batch\n",
        "    batch = examples[random_ind:(random_ind + batch_s)]\n",
        "    \n",
        "\n",
        "    batch_ordered_sentences.append([s[0] for s in batch])\n",
        "    batch_ordered_labels.append([s[1] for s in batch])\n",
        "\n",
        "    # Remove these examples rom the list.\n",
        "    del examples[random_ind:(random_ind + batch_s)]\n",
        "\n",
        "  return pad_batches(batch_ordered_sentences,batch_ordered_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2la_uxpR0evg",
        "outputId": "2fe38334-2746-4d79-bff6-ca4d36dfb4b1"
      },
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "#Create train batches\n",
        "train_data=select_batches(train_x,train_y,batch_size=30) #[train_inputs,train_att_mask,train_labels]\n",
        "\n",
        "#create valid batches\n",
        "valid_data=select_batches(valid_x,valid_y,batch_size=30) #valid_inputs,valid_att_mask,valid_labels\n",
        "\n",
        "train_data=select_batches(train_x,train_y,batch_size=30) #valid_inputs,valid_att_mask,valid_labels\n",
        "print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- 2.143338477611542 minutes ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atlTvDQZ0Udz"
      },
      "source": [
        "## **Bert for Model AnalyseSentiment**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIcWV-vs3Aoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc2fd6c-cd98-45a2-b9a4-437c29f09cad"
      },
      "source": [
        "#import bert model\n",
        "print(\"Defining model and config parameters\")\n",
        "config = BertConfig.from_pretrained('bert-base-uncased',num_labels=2) #specify number of labels [1,2]\n",
        "bert = BertModel.from_pretrained('bert-base-uncased',config=config)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining model and config parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWVQX0BN7-cW"
      },
      "source": [
        "class AnalyseSentimentBert(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Text classification model with BERT and \n",
        "  feedforward fully connected neural network with dropout and softmax layers\n",
        "  \"\"\"\n",
        "  def __init__(self, bert,freeze_bert=False):\n",
        "    super(AnalyseSentimentBert, self).__init__()\n",
        "    # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "    D_in, H, D_out = 768, 50, 2 #hidden size of BERT, hidden size of classifier, and number of labels\n",
        "    self.bert=bert\n",
        "    self.dropout=torch.nn.Dropout(0.1) #dropout layer\n",
        "    self.activation_relu=torch.nn.ReLU() #relu activation\n",
        "    self.linear1=torch.nn.Linear(D_in,H) #linear layer\n",
        "    self.linear2=torch.nn.Linear(H,D_out) #output layer\n",
        "    self.activation_softmax=torch.nn.Softmax(dim=1) #out probabilities\n",
        "    \n",
        "    # Freeze the BERT model\n",
        "    if freeze_bert:\n",
        "      for param in self.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "  def forward(self, input_ids,attention_masks):\n",
        "    #print(input_ids)\n",
        "    bert_out=self.bert(input_ids,attention_masks)\n",
        "    #print(bert_out)\n",
        "    last_hidden_state_cls=bert_out[0][:, 0, :]\n",
        "    x=self.linear1(last_hidden_state_cls)\n",
        "    x=self.activation_relu(x)\n",
        "    x=self.dropout(x)\n",
        "    x=self.linear2(x)\n",
        "    x=self.activation_softmax(x)\n",
        "\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bL2uralISr8"
      },
      "source": [
        "model=AnalyseSentimentBert(bert)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnspzvIeIq98"
      },
      "source": [
        "#model(train_data[0][2],train_data[1][2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWGvBIPPP9jA"
      },
      "source": [
        "## **Fine-tuning BERT model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw1ZaRKRPFid"
      },
      "source": [
        "#setting optimizer\n",
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(),lr = 1e-5)\n",
        "\n",
        "# define the loss function\n",
        "cross_entropy  = torch.nn.CrossEntropyLoss() #because we used logsoftmax as output activation + bineary classification task\n",
        "\n",
        "def train(model, traindata, validdata, epochs=1, evaluate=False, device=device):\n",
        "  \"\"\"Training for bert sentiment classifier\"\"\"\n",
        "  \n",
        "  min_loss = float('inf') #set symbolic valid loss\n",
        "  \n",
        "  for epoch_i in range(epochs): #loop on epoch number \n",
        "    \n",
        "    total_train_loss= 0.0\n",
        "    model.train() #training mode on\n",
        "\n",
        "    for batch in range(0, len(traindata)): #loop on every batch in traindata\n",
        "      \n",
        "      \n",
        "      #add batch features to GPU\n",
        "      batch_input_id=traindata[0][batch].to(device)\n",
        "      batch_att_mask=traindata[1][batch].to(device)\n",
        "      batch_labels=traindata[2][batch].to(device)\n",
        "\n",
        "      model.zero_grad()\n",
        "\n",
        "      predictions = model(batch_input_id, batch_att_mask) #make predictions\n",
        "      loss=cross_entropy(predictions,batch_labels) #calculate loss\n",
        "      \n",
        "      total_train_loss+=loss.item()\n",
        "\n",
        "      #norm of the gradients==1.0 to prevent \"exploding gradients\"\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      optimizer.step() #update parameters\n",
        "\n",
        "    #train loss for epoch\n",
        "    average_train_loss=total_train_loss/len(traindata)\n",
        "\n",
        "    if evaluate == True: #if evaluation mode \n",
        "      \n",
        "      \n",
        "      model.eval() #desactivate dropout layer\n",
        "      \n",
        "      total_eval_loss=0.0\n",
        "      \n",
        "      for batch in range(0, len(validdata)):\n",
        "\n",
        "        #add batch features to GPU\n",
        "        batch_input_id=validdata[0][batch].to(device)\n",
        "        batch_att_mask=validdata[1][batch].to(device)\n",
        "        batch_labels=validdata[2][batch].to(device)\n",
        "\n",
        "        with torch.no_grad(): #we don't need gradient descent\n",
        "          predictions=model(batch_input_id, batch_att_mask)\n",
        "          loss=cross_entropy(predictions,batch_labels) #calculate loss\n",
        "          total_eval_loss+=loss.item()\n",
        "      \n",
        "      #compute valid loss of the epoch\n",
        "      average_valid_loss=total_eval_loss/len(validdata)\n",
        "\n",
        "      if average_valid_loss < min_loss: #if valid loss is better\n",
        "        min_loss=average_valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_model.pt') #save model prameters\n",
        "\n",
        "      print(\"Train loss of epoch:\"+str(epoch_i)+\" ::: \"+str(average_train_loss))\n",
        "      print(\"Valid loss of epoch:\"+str(epoch_i)+\" ::: \"+str(average_valid_loss))\n",
        "      print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Acsv7Y9oe_IT",
        "outputId": "736531a4-72d0-40b7-c53b-2b070a18a0de"
      },
      "source": [
        "train(model,train_data,valid_data,evaluate=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train loss of epoch:0 ::: 0.6975581844647726\n",
            "Valid loss of epoch:0 ::: 0.682419498761495\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb4C_9liwUb1"
      },
      "source": [
        "## **Make Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oJ9D3JEvnQr",
        "outputId": "d661a509-fee5-442b-9b91-6663ac953b31"
      },
      "source": [
        "#call model\n",
        "path = 'saved_model.pt'\n",
        "model.load_state_dict(torch.load(path)) #load best model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzXwZg-D0V4H"
      },
      "source": [
        "#Tokenize for test data or never seen data\n",
        "def prepare(data,is_Test=True):\n",
        "  \"\"\"Process non seen datafile\n",
        "  Args:\n",
        "    data:data file\n",
        "    is_Test=bool flag for test file\n",
        "  Returns:\n",
        "    list (input ids,attention masks) assigned to file\"\"\"\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "  if is_Test==False: #if new data/string\n",
        "    data=read_data_file(data,labelled=False)\n",
        "  \n",
        "  for sent in data:\n",
        "    tokenize_data = tokenizer(sent,\n",
        "                                     add_special_tokens=True,\n",
        "                                     truncation=True,\n",
        "                                     max_length = 100,\n",
        "                                     pad_to_max_length=True,\n",
        "                                     return_attention_mask=True)\n",
        "    input_ids.append(tokenize_data['input_ids'])\n",
        "    attention_masks.append(tokenize_data['attention_mask'])\n",
        "\n",
        "\n",
        "  input_ids = torch.tensor(input_ids)\n",
        "  attention_masks = torch.tensor(attention_masks)\n",
        "  return [input_ids, attention_masks]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMWSaAxgyr-2",
        "outputId": "d742ee82-d0cc-43e1-8113-dd7230519c39"
      },
      "source": [
        "#try sur test data\n",
        "\n",
        "test_data=prepare(test_x[:200])\n",
        "test_y=torch.tensor(test_y[:200])\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  preds = model(test_data[0].to(device), test_data[1].to(device)) #make prediction\n",
        "  preds = preds.clone().detach().cpu().numpy()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACXEixuCzYi-",
        "outputId": "43b1fd89-b876-49bb-c34d-670320151752"
      },
      "source": [
        "\n",
        "final_preds = np.argmax(preds,axis=1)\n",
        "\n",
        "print(classification_report(test_y, final_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      1.00      0.77       126\n",
            "           1       0.00      0.00      0.00        74\n",
            "\n",
            "    accuracy                           0.63       200\n",
            "   macro avg       0.32      0.50      0.39       200\n",
            "weighted avg       0.40      0.63      0.49       200\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vTKL5YfKbOD"
      },
      "source": [
        "def pred_sequence(seq):\n",
        "\n",
        "  cleaned=preprocess(seq)\n",
        "  truncated=tokenizer(cleaned,add_special_tokens=True,truncation=True,max_length = 100,pad_to_max_length=True,return_attention_mask=True,return_tensors='pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    pred = model(truncated['input_ids'].to(device), truncated['attention_mask'].to(device))\n",
        "  if np.argmax(pred)==1:\n",
        "    return 2\n",
        "  elif np.argmax(pred)==0:\n",
        "    return 1\n",
        "  else:\n",
        "    print(\"Problem in classification\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "213N1IV8LAY_",
        "outputId": "a9865e93-9330-4800-b1b9-a39493a2542e"
      },
      "source": [
        "text=\"I got 'new' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\nI took the tire over to Flynn's and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he'd give me a new tire \\this time\\. \\nI will never go back to Flynn's b/c of the way this guy treated me and the simple fact that they gave me a used tire!\"\n",
        "print(pred_sequence(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}